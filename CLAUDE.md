# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

**weather_tools** is a Python CLI tool and library for accessing and processing Australian SILO (Scientific Information for Land Owners) climate data. It supports both API-based queries and local NetCDF file processing.

[SILO API Guide](https://www.longpaddock.qld.gov.au/silo/api-documentation/guide/)
[SILO API reference](https://www.longpaddock.qld.gov.au/silo/api-documentation/reference/)


**Core Features:**
- SILO API client with Pydantic validation for type-safe queries
- Local NetCDF file support via xarray/pandas
- Cloud-Optimized GeoTIFF (COG) support with spatial subsetting
- CLI interface built with Typer
- Two dataset types: PatchedPoint (station data) and DataDrill (gridded data at 0.05Â° resolution)

## Development Commands

### Environment Setup
```bash
# Install dependencies (using uv package manager)
uv sync

```

### Testing
```bash
# Run all tests
uv run pytest tests/

# Run simple/fast tests only
uv run pytest tests/test_read_silo_simple.py

# Run with coverage report
uv run pytest tests/ --cov=src --cov-report=term-missing

# Run specific test
uv run pytest tests/test_read_silo_simple.py::test_extract_single_point -v

# Skip integration tests
uv run pytest tests/ -m "not integration"
```

**Note:** Tests require SILO data in `~/Developer/DATA/silo_grids/`. If unavailable, tests auto-skip.

### Code Quality
```bash
# Format code (using ruff)
uv run ruff format .

# Lint code
uv run ruff check .

# Run bandit security checks
uv run bandit -r src/

```

### CLI Usage During Development
```bash
# Run CLI without installation
uv run weather-tools --help

# Query SILO API (requires SILO_API_KEY environment variable)
export SILO_API_KEY="your.email@example.com"
uv run weather-tools silo patched-point --station 30043 --start-date 20230101 --end-date 20230131 --var R --var X

# Work with local NetCDF files
uv run weather-tools local extract --lat -27.5 --lon 153.0 --start-date 2020-01-01 --end-date 2020-12-31

# Download SILO data from AWS S3
uv run weather-tools local download --var daily --start-year 2020 --end-year 2023

# Download GeoTIFF files with spatial clipping
uv run weather-tools geotiff download --var daily_rain \
    --start-date 2023-01-01 --end-date 2023-01-31 \
    --bbox 150.5 -28.5 154.0 -26.0
```

### Download SILO Data
```bash
# Download daily variables for 2020-2023
weather-tools local download --var daily --start-year 2020 --end-year 2023

# Download specific variables
weather-tools local download --var daily_rain --var max_temp \
    --start-year 2022 --end-year 2023

# Download to custom directory with force overwrite
weather-tools local download --var monthly \
    --start-year 2020 --end-year 2023 \
    --silo-dir /data/silo_grids --force
```

## Architecture

### Core Modules

**`silo_models.py`** - Pydantic models for type-safe API interactions
- `SiloDataset`, `SiloFormat`, `ClimateVariable` enums define API options
- `SiloDateRange` validates dates (YYYYMMDD format, 1889-present)
- `AustralianCoordinates` validates lat/lon within Australian bounds
- `PatchedPointQuery` and `DataDrillQuery` represent API requests
- `SiloResponse` wraps API responses with metadata
- All query models have `to_api_params()` method to convert to HTTP parameters

**`silo_api.py`** - HTTP client for SILO API
- `SiloAPI` class handles all API communication
- Retry logic with exponential backoff for transient errors
- Optional response caching (disabled by default)
- `log_level` controls constructed URL emission (set to `DEBUG` for request details)
- Two query methods: `query_patched_point()` and `query_data_drill()` accept Pydantic models
- Convenience methods (`get_station_data()`, `get_gridded_data()`, `search_stations()`) accept simple string arguments
- `_response_to_dataframe()` converts API responses to pandas DataFrames
- Error handling via `SiloAPIError` exception

**`silo_variables.py`** - Central registry for climate variables
- Maps between API codes (R, X, N) and NetCDF names (daily_rain, max_temp)
- Variable metadata including units, start years, descriptions
- Preset groups for common variable collections ("daily", "monthly", etc.)
- Used by both API client and download module for consistency

**`download_silo.py`** - NetCDF file downloader
- Downloads files from AWS S3 public data (`s3-ap-southeast-2.amazonaws.com/silo-open-data`)
- Constructs URLs: `{base}/annual/{variable}/{year}.{variable}.nc`
- Rich progress bars with download speed and ETA
- Validates year ranges based on variable availability
- Skips existing files by default (override with `--force`)
- Creates directory structure compatible with `read_silo_xarray()`

**`read_silo_xarray.py`** - Local NetCDF file loader
- `read_silo_xarray()` loads local SILO gridded data into xarray datasets
- Supports variable presets: "daily", "monthly", or explicit variable lists
- Uses centralized preset definitions from `silo_variables.py`
- Default data directory: `~/Developer/DATA/silo_grids/`
- Expected structure: `{variable_name}/{year}.{variable_name}.nc`

**`silo_geotiff.py`** - Cloud-Optimized GeoTIFF support
- `construct_daily_url()` and `construct_monthly_url()` - Build URLs for SILO GeoTIFF files on S3
- `read_cog()` - Read COG data for Point/Polygon geometries using HTTP range requests
- `download_geotiff_with_subset()` - Download GeoTIFF files with optional spatial clipping
- `read_geotiff_timeseries()` - Read time series data (streaming or disk-cached)
- `download_geotiff_range()` - Batch download GeoTIFFs with progress tracking
- Leverages COG features: partial spatial reads, overview pyramids, HTTP range requests
- Supports both in-memory streaming (no disk usage) and disk caching workflows
- Error handling via `SiloGeoTiffError` exception

### Logging and Console Output

Use the shared helpers in `weather_tools.logging_utils` for all CLI and SDK messaging. Call `configure_logging()` once (for example in `cli.main()`) to attach the Rich-backed handler that writes through the shared console. When you need a console object for progress bars, call `get_console()`, but send user-facing messages through the standard logging APIs rather than `console.print()`. The configured handler already understands Rich markup such as `[green]...[/green]`, so existing styling keeps working. Avoid configuring logging manually in new modules; reuse these helpers to maintain consistent output formatting.

**`cli.py`** - Typer-based command-line interface
- Main app with three subcommands: `silo` (API queries), `local` (NetCDF files), `geotiff` (COG files)
- `silo` commands: `patched-point`, `data-drill`, `search`
- `local` commands: `extract`, `info`, `download`
- `geotiff` commands: `download` (with optional --bbox or --geometry clipping)
- Entry point: `main()` function registered as `weather-tools` script

### Data Flow

**API Query Flow:**
1. User creates Pydantic query model (`PatchedPointQuery` or `DataDrillQuery`)
2. Query model validates all parameters (coordinates, dates, variables)
3. `SiloAPI` converts query to HTTP params via `to_api_params()`
4. API makes request with retry logic and optional caching
5. Response parsed into `SiloResponse` with format/dataset metadata
6. Can convert to DataFrame via `_response_to_dataframe()` or access raw data

**Local File Flow:**
1. `read_silo_xarray()` scans directory for NetCDF files matching variable names
2. Opens files with xarray, concatenates across years
3. Returns xarray Dataset with time/lat/lon coordinates
4. User can use xarray's `.sel()` for location/time slicing, then `.to_dataframe()`

**GeoTIFF Flow:**
1. User provides Point or Polygon geometry (shapely) and date range
2. `read_geotiff_timeseries()` or `read_cog()` constructs URLs using `construct_daily_url()`
3. COG files read directly from S3 using HTTP range requests (rasterio)
4. `geometry_window()` calculates spatial subset to read
5. Only requested pixels transferred via HTTP range requests (COG efficiency)
6. Returns numpy arrays with time dimension, ready for analysis
7. Optional: Files cached to disk for reuse (via `save_to_disk=True`)

### Key Design Patterns

**Type Safety via Pydantic:**
- All API parameters validated before requests
- Enums prevent invalid variable codes, formats, or datasets
- Date ranges validated for format and logical ordering
- Australian coordinate bounds enforced

**Dual Interface:**
- Low-level: Use Pydantic models for full type safety and validation
- High-level: Use convenience methods with simple strings for quick queries
- CLI wraps both approaches for command-line access

**Error Handling:**
- `SiloAPIError` for all API-related failures (HTTP errors, SILO-specific errors)
- Retry logic only for transient errors (timeouts, connection issues)
- No retries for validation errors or 4xx HTTP responses
- Tests auto-skip when data unavailable rather than failing

## Important Notes

### SILO API Requirements
- API key is an email address, set via `SILO_API_KEY` environment variable or passed to `SiloAPI(api_key="...")`
- PatchedPoint and DataDrill use different endpoints but share parameter structure
- Some formats (NEAR, NAME, ID) only work with PatchedPoint dataset
- Date format is always YYYYMMDD (no dashes or slashes)
- Variable codes are single letters (R=rainfall, X=max_temp, N=min_temp, etc.)

### Local Data Structure
- NetCDF files must be in `{variable_name}/{year}.{variable_name}.nc` structure
- Variables: `daily_rain`, `monthly_rain`, `max_temp`, `min_temp`, `evap_syn`, `evap_pan`, `radiation`, `vp`
- Files use GDA94 coordinate system with lat/lon dimensions
- Time dimension named "time" with datetime64 dtype

### Module Dependencies
- CLI imports `silo_api`, `read_silo_xarray`, `download_silo`, and `silo_geotiff` modules
- `silo_api` imports `silo_models` for all type definitions
- `read_silo_xarray` imports `silo_variables` for preset expansion
- `download_silo` imports `silo_variables` for variable metadata
- `silo_geotiff` imports `silo_variables` for variable metadata and preset expansion
- `silo_geotiff` uses `rasterio` for COG reading and `shapely` for geometry handling
- No circular dependencies

### Testing Strategy
- Tests split into simple (fast) and comprehensive suites
- Use fixtures to check for data availability before running
- Always close xarray datasets with `ds.close()` to free memory
- Integration tests marked with `pytest.mark.integration`
- Mock API responses for `silo_api` tests to avoid real API calls

### NetCDF Downloads
- Files downloaded from AWS S3 public data (no authentication required)
- Daily variable files are ~410MB each (365-366 daily grids)
- Monthly rainfall files are ~14MB each (12 monthly grids)
- S3 does not support partial downloads - entire files must be downloaded
- Geographic filtering should be done after download using xarray's `.sel()` method
- Variable availability varies by year (most: 1889+, MSLP: 1957+, evap_pan: 1970+)

### GeoTIFF Structure and Usage
- **URL Pattern**: `https://s3-ap-southeast-2.amazonaws.com/silo-open-data/Official/daily/{variable}/{year}/{YYYYMMDD}.{variable}.tif`
- **File Organization** (when cached): `{cache_dir}/{variable}/{year}/{YYYYMMDD}.{variable}.tif`
- **COG Benefits**: Only download pixels you need via HTTP range requests (much faster than full file)
- **Date Format**: GeoTIFF uses YYYY-MM-DD (unlike NetCDF which uses years)
- **CRS**: All GeoTIFFs are in EPSG:4326 (WGS84 lat/lon)
- **Geometry Support**: Accept shapely Point or Polygon objects
- **Workflows**:
  - **Streaming**: `save_to_disk=False` reads directly from S3 without caching (great for one-off queries)
  - **Caching**: `save_to_disk=True` downloads to local disk for reuse
- **CLI vs Python API**:
  - CLI: Download command only (saves to disk with optional spatial clipping)
  - Python API: Full functionality including streaming reads and Point/Polygon queries

### Common Pitfalls
- Forgetting to set `SILO_API_KEY` environment variable causes initialization error
- PatchedPoint queries require `station_code`, DataDrill queries require `coordinates`
- Date ranges must include both start_date and end_date for data queries
- Local file paths assume specific directory structure - missing files cause FileNotFoundError
- xarray datasets consume significant memory - close when done or work with subsets
- NetCDF download command uses year range (--start-year/--end-year), GeoTIFF uses dates (--start-date/--end-date)
- GeoTIFF geometries use (lon, lat) order, not (lat, lon) - `Point(153.0, -27.5)` is Brisbane
- GeoTIFF CRS must be EPSG:4326 - other projections will raise `SiloGeoTiffError`
- Cannot specify both --bbox and --geometry in GeoTIFF download - they are mutually exclusive
- Some GeoTIFF files may return 404 if data is unavailable for that date (handled gracefully, logs warning)
